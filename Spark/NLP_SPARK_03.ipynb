{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZAb_M7dT3eX",
        "outputId": "29233144-0307-4d81-fe2f-dc5607cb1c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7RIEzGMT1ay",
        "outputId": "c56513ac-c242-4d2a-c888-34d1d867144d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 42 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 49.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=02dfb333b36c504caf977780650dc8e98eedd30a0ec86ccc9df08a061a59eb14\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWhqt9axrxW8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"ModelTraining\")\n",
        "    .config(\"spark.executor.memory\", \"12g\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = 30\n",
        "pd.options.display.max_colwidth = 150\n",
        "\n",
        "schema = \"polarity FLOAT, id LONG, date_time TIMESTAMP, query STRING, user STRING, text STRING\"\n",
        "timestampformat = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
        "\n",
        "IN_PATH = \"/content/drive/MyDrive/sentiment-140-training-data/CLEAN\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/sentiment-140-training-data/MODEL\"\n",
        "\n",
        "spark_reader = spark.read.schema(schema)\n",
        "\n",
        "\n",
        "df_clean = spark.read.parquet(IN_PATH).cache()\n",
        "\n",
        "df_clean = (\n",
        "    df_clean\n",
        "    # Remove all numbers\n",
        "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \"))\n",
        "    # Remove all double/multiple spaces\n",
        "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \" +\", \" \"))\n",
        "    # Remove leading and trailing whitespaces\n",
        "    .withColumn(\"text\", f.trim(f.col(\"text\")))\n",
        "    # Remove repeated letters\n",
        "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"((\\w+)\\1{1,})\", \"\"))\n",
        "    \n",
        "    # Ensure we don't end up with empty rows\n",
        "    .filter(\"text != ''\")\n",
        ")\n",
        "\n",
        "data = df_clean.select(\"text\", \"polarity\").coalesce(3).cache()\n",
        "\n",
        "data = data.na.drop()\n",
        "\n",
        "data = data.sample(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5UkGabGrxXN",
        "outputId": "1f201d48-dd04-4850-cf01-8982ecbeb39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159283\n"
          ]
        }
      ],
      "source": [
        "print( data.count() )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupBy(\"polarity\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TueayQdKkMe9",
        "outputId": "7def293a-009c-4b35-8308-7cc9092aa384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|polarity|count|\n",
            "+--------+-----+\n",
            "|     0.0|79525|\n",
            "|     4.0|79758|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data.select(\"text\").collect()"
      ],
      "metadata": {
        "id": "_zWgdLMkZ-hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgWTbRD2rxXT"
      },
      "outputs": [],
      "source": [
        "(training_data, validation_data, test_data) = data.randomSplit([0.7, 0.2, 0.1], seed=2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVrBDhxkrxXU"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.feature import (\n",
        "    StopWordsRemover,\n",
        "    Tokenizer,\n",
        "    HashingTF,\n",
        "    IDF,\n",
        ")\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenizer converts input string to lowercase and then splits it by white spaces.\n",
        "# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer\n",
        "# Params:\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words1\")\n",
        "\n",
        "# A feature transformer that filters out stop words from input.\n",
        "# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover\n",
        "# Params:\n",
        "stopwords_remover = StopWordsRemover(\n",
        "    inputCol=\"words1\",\n",
        "    outputCol=\"words2\",\n",
        "    stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"),  # English stopwords\n",
        ")\n",
        "\n",
        "# Maps a sequence of terms to their term frequencies using the hashing trick\n",
        "# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n",
        "# Params:\n",
        "hashing_tf = HashingTF(inputCol=\"words2\", outputCol=\"term_frequency\")\n",
        "\n",
        "# Compute the Inverse Document Frequency (IDF) given a collection of documents\n",
        "# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.IDF\n",
        "# Params:\n",
        "idf = IDF(\n",
        "    inputCol=\"term_frequency\",\n",
        "    outputCol=\"features\",\n",
        "    minDocFreq=5,  # minDocFreq: remove sparse terms\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"polarity\",featuresCol=\"features\")\n",
        "\n",
        "semantic_analysis_pipeline = Pipeline(\n",
        "    stages=[tokenizer, stopwords_remover, hashing_tf, idf,lr]\n",
        ")\n",
        "\n",
        "semantic_analysis_model = semantic_analysis_pipeline.fit(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_analysis_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImqX9nmPA75i",
        "outputId": "376b7c1e-b099-4ed8-8018-2133d26830cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PipelineModel_a42a37816142"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFcg8ZC9rxXW",
        "outputId": "03307431-ea12-4eda-ee1a-b236c1d6a78e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 196 ms, sys: 38.9 ms, total: 235 ms\n",
            "Wall time: 1.23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "trained_df = semantic_analysis_model.transform(training_data)\n",
        "val_df = semantic_analysis_model.transform(validation_data)\n",
        "test_df = semantic_analysis_model.transform(test_data)\n",
        "\n",
        "#trained_df.show()\n",
        "#val_df.show()\n",
        "#test_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I-cB7wNrxXX"
      },
      "outputs": [],
      "source": [
        "# StopWordsRemover.loadDefaultStopWords(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m37DNkJ2rxXX",
        "outputId": "848ccd38-1f9b-4ff6-da24-6683dd0017fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Data:\n",
            "Accuracy: 75.07027%\n",
            "Testing Data:\n",
            "Accuracy: 75.34869%\n",
            "CPU times: user 95.1 ms, sys: 4.72 ms, total: 99.9 ms\n",
            "Wall time: 12.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"polarity\", metricName=\"accuracy\")\n",
        "accuracy_val = evaluator.evaluate(val_df)\n",
        "accuracy_test = evaluator.evaluate(test_df)\n",
        "print(\"Validation Data:\")\n",
        "print(f\"Accuracy: {accuracy_val*100:.5f}%\")\n",
        "print(\"Testing Data:\")\n",
        "print(f\"Accuracy: {accuracy_test*100:.5f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vcMzfKd9z76L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvA5UYxYrxXY"
      },
      "outputs": [],
      "source": [
        "#final_model = semantic_analysis_pipeline.fit(data)\n",
        "semantic_analysis_model.save(MODEL_PATH+\"NEW_MODEL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8ty-wJNrxXY"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP_SPARK_03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}